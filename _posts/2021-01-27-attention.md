---
title: "Can Attention Replace Convolution?"
date: 2021-01-27
tags: [attention, deep-learning]
permalink: /attention/
mathjax: true
---

# Attention
Attention mechanisms have gained the interest of a large number of researchers. 
The [Transformer](https://arxiv.org/abs/1706.03762) introduced by Vaswani et al. in 2017 is the key in nearly all state of the art NLP models (*BERT*, *Transformer-XL*).


